{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":66653,"databundleVersionId":7500999,"sourceType":"competition"}],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-19T13:34:00.416931Z","iopub.execute_input":"2024-03-19T13:34:00.417317Z","iopub.status.idle":"2024-03-19T13:34:00.428549Z","shell.execute_reply.started":"2024-03-19T13:34:00.417287Z","shell.execute_reply":"2024-03-19T13:34:00.427392Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"/kaggle/input/pii-detection-removal-from-educational-data/sample_submission.csv\n/kaggle/input/pii-detection-removal-from-educational-data/train.json\n/kaggle/input/pii-detection-removal-from-educational-data/test.json\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ntokenizer_gpt2 = AutoTokenizer.from_pretrained('gpt2')\n'''\ntransformers 라이브러리의 tokenizer 객체는 텍스트를 토큰화한 후, 여러 형태의 정보를 포함하는 딕셔너리를 반환합니다. \n이 딕셔너리 내에서 input_ids는 토큰화된 텍스트의 각 토큰에 대응하는 고유한 숫자 ID들의 리스트입니다.\n'''\ntokenizer_gpt2(\"creative\")","metadata":{"execution":{"iopub.status.busy":"2024-03-19T14:24:30.032413Z","iopub.execute_input":"2024-03-19T14:24:30.032817Z","iopub.status.idle":"2024-03-19T14:24:30.444262Z","shell.execute_reply.started":"2024-03-19T14:24:30.032789Z","shell.execute_reply":"2024-03-19T14:24:30.443150Z"},"trusted":true},"execution_count":39,"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"{'input_ids': [20123, 425], 'attention_mask': [1, 1]}"},"metadata":{}}]},{"cell_type":"code","source":"import json\n\n\ntrain_file_path = '/kaggle/input/pii-detection-removal-from-educational-data/train.json'\n\nwith open(train_file_path, 'r') as file:\n    train = json.load(file)\n\ndef retokenize(tokens_org, labels_org, trailing_whitespaces, tokenizer):\n    '''\n    기존 데이터셋의 trailing_whitespaces 정보를 이용해\n    원본토큰 앞에 공백이 있는 경우, 공백을 추가한 후\n    공백이 추가된 토큰을 토크나이저에 전달해 토크나이징한 뒤\n    결과로 나온 토큰ID 리스트인 new_tokens를 transformed_tokens 리스트에 추가하고\n    동시에 해당 토큰의 개수만큼 기존 레이블을 복제해 ransformed_labels 리스트에 추가하는 함수\n    '''\n    transformed_tokens = []\n    transformed_labels = []\n\n    lws = False # Leading whitespace\n    '''\n\n    각 토큰 앞에 공백이 있을 경우 (즉, 해당 토큰이 새로운 단어의 시작을 나타내는 경우) 그 공백을 추가하고, \n    이렇게 조정된 텍스트를 토크나이저에 전달하여 토큰화한 뒤, 결과로 나온 토큰 ID들(input_ids)을 new_tokens 변수에 저장\n    '''\n    for token, label, tws in zip(tokens_org, labels_org, trailing_whitespaces):\n        new_tokens = tokenizer((' ' if lws else '') + token)['input_ids']\n        transformed_tokens += new_tokens\n        transformed_labels += [label] * len(new_tokens)\n        lws = tws\n\n    return transformed_tokens, transformed_labels","metadata":{"execution":{"iopub.status.busy":"2024-03-19T13:37:49.543486Z","iopub.execute_input":"2024-03-19T13:37:49.543954Z","iopub.status.idle":"2024-03-19T13:38:02.625262Z","shell.execute_reply.started":"2024-03-19T13:37:49.543924Z","shell.execute_reply":"2024-03-19T13:38:02.624174Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c06094602fdb46d9a248e970b66f3177"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"228d2385baf84abf810db503619a1a2c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de111770c6bd4c50ae9931326bee87b9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"013f85f7108f4d40b3a311ba6bd025bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"130ba98571d64d1583e3d40217ac1d9a"}},"metadata":{}}]},{"cell_type":"code","source":"full_text = train[0]['full_text']\ntrailing_whitespace = train[0]['trailing_whitespace']\noriginal_tokens, original_labels = train[0]['tokens'], train[0]['labels']","metadata":{"execution":{"iopub.status.busy":"2024-03-19T13:38:32.967328Z","iopub.execute_input":"2024-03-19T13:38:32.967887Z","iopub.status.idle":"2024-03-19T13:38:32.973289Z","shell.execute_reply.started":"2024-03-19T13:38:32.967854Z","shell.execute_reply":"2024-03-19T13:38:32.972236Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"new_tokens, new_labels = retokenize(\n    original_tokens, \n    original_labels, \n    trailing_whitespace, \n    tokenizer_gpt2)","metadata":{"execution":{"iopub.status.busy":"2024-03-19T13:39:55.901646Z","iopub.execute_input":"2024-03-19T13:39:55.902643Z","iopub.status.idle":"2024-03-19T13:39:55.953842Z","shell.execute_reply.started":"2024-03-19T13:39:55.902608Z","shell.execute_reply":"2024-03-19T13:39:55.952793Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"### 원본 토큰과 retokenize 함수 결과 비교","metadata":{}},{"cell_type":"code","source":"print(\"원본토큰\", end=\" \")\nfor token, label in zip(original_tokens[:8], original_labels[:8]):\n    print(token, label, \"|\", end= \" \")\nprint(); print(\"재토큰화\", end=\" \")    \nretokenized_tokens = tokenizer_gpt2.convert_ids_to_tokens(new_tokens[:10])\nfor new_token, new_label in zip(retokenized_tokens, new_labels[:10]):\n    print(new_token, new_label, \"|\", end= \" \")","metadata":{"execution":{"iopub.status.busy":"2024-03-19T14:36:18.830492Z","iopub.execute_input":"2024-03-19T14:36:18.831173Z","iopub.status.idle":"2024-03-19T14:36:18.839156Z","shell.execute_reply.started":"2024-03-19T14:36:18.831142Z","shell.execute_reply":"2024-03-19T14:36:18.838010Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"원본토큰 Design O | Thinking O | for O | innovation O | reflexion O | - O | Avril O | 2021 O | \n재토큰화 Design O | ĠThinking O | Ġfor O | Ġinnovation O | Ġreflex O | ion O | - O | Av O | ril O | Ġ2021 O | ","output_type":"stream"}]},{"cell_type":"markdown","source":"### retokenize 함수결과와 gpt2 오리지널 토크나이저 결과 비교","metadata":{}},{"cell_type":"markdown","source":"retokinze 함수를 쓰면 원본토큰 중 \\n\\n 이 ĊĊ 으로 토크나이징 되는데  \nfull_text를 그대로 gpt2 토크나이저에 넣어 토크나이징 하면 \\n\\n 이 분리되어 별도의 Ċ로 토크나이징 된다.  ","metadata":{}},{"cell_type":"code","source":"'''\nĠ 문자는 GPT-2 토큰화에서 사용되는 특수 문자로, 단어의 시작을 나타내며 앞에 공백이 있음을 의미\nĊ 문자는 \\n 줄바꿈을 의미\n'''\nprint(tokenizer_gpt2.convert_ids_to_tokens(198))\ntokenizer_gpt2.decode([198])","metadata":{"execution":{"iopub.status.busy":"2024-03-19T14:43:59.850732Z","iopub.execute_input":"2024-03-19T14:43:59.851162Z","iopub.status.idle":"2024-03-19T14:43:59.859688Z","shell.execute_reply.started":"2024-03-19T14:43:59.851123Z","shell.execute_reply":"2024-03-19T14:43:59.858350Z"},"trusted":true},"execution_count":76,"outputs":[{"name":"stdout","text":"Ċ\n","output_type":"stream"},{"execution_count":76,"output_type":"execute_result","data":{"text/plain":"'\\n'"},"metadata":{}}]},{"cell_type":"code","source":"fully_tokenized_gpt2 = tokenizer_gpt2(full_text)['input_ids']\nfully_retokenized_tokens = tokenizer_gpt2.convert_ids_to_tokens(fully_tokenized_gpt2[11:20])\nretokenized_tokens = tokenizer_gpt2.convert_ids_to_tokens(new_tokens[11:20])\nfor new_token, full_token in zip(retokenized_tokens, fully_retokenized_tokens):\n    print(new_token, full_token)","metadata":{"execution":{"iopub.status.busy":"2024-03-19T14:46:00.586628Z","iopub.execute_input":"2024-03-19T14:46:00.586992Z","iopub.status.idle":"2024-03-19T14:46:00.595455Z","shell.execute_reply.started":"2024-03-19T14:46:00.586965Z","shell.execute_reply":"2024-03-19T14:46:00.594413Z"},"trusted":true},"execution_count":78,"outputs":[{"name":"stdout","text":"N N\nath ath\nal al\nie ie\nĠSy ĠSy\nlla lla\nĊĊ Ċ\nChall Ċ\nenge Chall\n","output_type":"stream"}]},{"cell_type":"code","source":"transformed_text = tokenizer_gpt2.decode(new_tokens)\nprint(f'original_text:\\n\\n{transformed_text[:89]}')","metadata":{"execution":{"iopub.status.busy":"2024-03-19T14:39:10.415615Z","iopub.execute_input":"2024-03-19T14:39:10.416027Z","iopub.status.idle":"2024-03-19T14:39:10.429542Z","shell.execute_reply.started":"2024-03-19T14:39:10.415994Z","shell.execute_reply":"2024-03-19T14:39:10.428442Z"},"trusted":true},"execution_count":64,"outputs":[{"name":"stdout","text":"original_text:\n\nDesign Thinking for innovation reflexion-Avril 2021-Nathalie Sylla\n\nChallenge & selection\n","output_type":"stream"}]},{"cell_type":"code","source":"print(f'full_text:\\n')\nfull_text[:89]","metadata":{"execution":{"iopub.status.busy":"2024-03-19T14:40:26.228849Z","iopub.execute_input":"2024-03-19T14:40:26.229234Z","iopub.status.idle":"2024-03-19T14:40:26.236794Z","shell.execute_reply.started":"2024-03-19T14:40:26.229205Z","shell.execute_reply":"2024-03-19T14:40:26.235411Z"},"trusted":true},"execution_count":68,"outputs":[{"name":"stdout","text":"full_text:\n\n","output_type":"stream"},{"execution_count":68,"output_type":"execute_result","data":{"text/plain":"'Design Thinking for innovation reflexion-Avril 2021-Nathalie Sylla\\n\\nChallenge & selection'"},"metadata":{}}]},{"cell_type":"markdown","source":"### 토크나이징 결과 비교","metadata":{}},{"cell_type":"code","source":"print(\"Num tokens originally:\", len(original_tokens))\nprint(\"Num tokens fully gpt2 tokenized:\", len(fully_tokenized_gpt2))\nprint(\"Num tokens token-by-token gpt2 tokenized:\", len(new_tokens))","metadata":{"execution":{"iopub.status.busy":"2024-03-19T13:59:54.883512Z","iopub.execute_input":"2024-03-19T13:59:54.883891Z","iopub.status.idle":"2024-03-19T13:59:54.889635Z","shell.execute_reply.started":"2024-03-19T13:59:54.883863Z","shell.execute_reply":"2024-03-19T13:59:54.888448Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Num tokens originally: 753\nNum tokens fully gpt2 tokenized: 833\nNum tokens token-by-token gpt2 tokenized: 849\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}